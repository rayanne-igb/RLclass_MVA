{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42c17b7-ee43-46a1-94d0-114b7f08cce9",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926eb01f-4548-489a-b60a-845d5dc577a8",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 5: Continuous actions in DQN algorithms</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc53bd-3f2e-401a-80c3-e330ef2aecad",
   "metadata": {},
   "source": [
    "In previous chapters, we introduced function approximation within value iteration methods, which enabled learning optimal value functions in very large state spaces. As we were deriving the corresponding algorithms, we always retained an implicit policy, as that which was greedy with respect to the value function. This was easy since all the challenges we faced had only a small number of discrete actions. When the action space turns continuous (or when there are too many actions for an efficient enumeration), writing $\\pi(s) \\in \\arg\\max_a Q(s,a)$ does not translate as easily into an implementable policy. Deciding how to choose actions boils down to solving a separate optimization problem in every state, which can quickly become quite impractical. In the present chapter, we make the policy explicit, and build upon the actor-critic architecture to derive approximate value iteration algorithms for continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83fc81-7b5c-4f66-947f-bc06a59db43f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**   \n",
    "By the end of this chapter, you should be able to:\n",
    "- explain the deterministic policy gradient theorem\n",
    "- implement a DDPG algorithm and discuss its properties\n",
    "- implement TD3, motivate it, and discuss its properties\n",
    "- explain the derivation of soft policy iteration and soft actor-critic\n",
    "- implement a SAC algorithm with fixed temperature\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13278abb-9b7b-415b-8cc2-8f94357bbe64",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "We will be playing with different continuous environments in this chapter. Gymnasium provides a number of such environments, for example through the [Box2D](https://gymnasium.farama.org/environments/box2d/) and [MuJoCo](https://gymnasium.farama.org/environments/mujoco/) families.\n",
    "\n",
    "To keep things computationally light, we will mostly play with the [inverted pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) environment for the MuJoCo suite. Some exercises along the chapter will open towards more difficult benchmarks. Feel free to try them (or others), to build a better sense of how algorithms work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd4168-9ef4-4d5d-9334-b241b6eca341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from tqdm import trange\n",
    "\n",
    "#test_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array_list\")\n",
    "test_env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array_list\")\n",
    "s,_ = test_env.reset()\n",
    "for t in range(1000):\n",
    "    a = test_env.action_space.sample()\n",
    "    s2,r,d,trunc,_ = test_env.step(a)\n",
    "    s = s2\n",
    "    if d:\n",
    "        break\n",
    "\n",
    "save_video(test_env.render(), \"videos\", fps=test_env.metadata[\"render_fps\"], name_prefix=\"random_policy\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d86baf-52d7-44ee-9e48-79d8896029fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/random_policy-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d366efb-15fe-416c-9446-082f93f26fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "#test_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "test_env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
    "s,_ = test_env.reset()\n",
    "returns = []\n",
    "for _ in trange(50):\n",
    "    cumulated_reward = 0\n",
    "    s,_ = test_env.reset()\n",
    "    for t in range(1000):\n",
    "        a = test_env.action_space.sample()\n",
    "        s2,r,d,trunc,_ = test_env.step(a)\n",
    "        cumulated_reward += r\n",
    "        s = s2\n",
    "        if d:\n",
    "            break\n",
    "    returns.append(cumulated_reward)\n",
    "\n",
    "print(np.mean(returns))\n",
    "print(np.std(returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be952fd8-30b9-455b-81b6-b2ffddf1eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_env.action_space)\n",
    "print(test_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e25f07-942f-4113-aa7e-138c3da98a5b",
   "metadata": {},
   "source": [
    "# Deep deterministic policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98bc58-c921-4f0b-8f51-f72e97580156",
   "metadata": {},
   "source": [
    "## The deterministic policy gradient theorem\n",
    "\n",
    "Let us, once again, restart from approximate value iteration (AVI) as a sequence of risk minimization problems.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Approximate value iteration as a sequence of risk minimization problems**  \n",
    "$$\\pi_n \\in \\mathcal{G} Q_n,$$\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( Q(s,a;\\theta) - G^{\\pi_n}_1(s,a,Q_n) \\right)^2 \\right],$$\n",
    "$$\\theta_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta),$$\n",
    "$$Q_{n+1}(s,a) = Q(s,a;\\theta_{n+1}).$$\n",
    "</div>\n",
    "\n",
    "For deterministic policies, we have\n",
    "$$\\pi \\in \\mathcal{G} Q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{a \\in A} \\left[Q(s,a)\\right], \\forall s\\in S.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec74947-21b7-4157-a1cc-4e0f8a65834e",
   "metadata": {},
   "source": [
    "Finding $\\pi \\in \\mathcal{G}Q$ was relatively easy as long as there were few, discrete actions. But when actions are continuous, solving a $\\max_a$ problem is a continuous optimization problem on which we have little knowledge.\n",
    "\n",
    "Let us turn to what we called \"weak optimality\" in a previous chapter, that is, given a distribution $\\rho_0(s)$ on starting states, the search for a policy that maximizes $J(\\pi) = \\mathbb{E}_{s\\sim \\rho_0} [V^\\pi(s)]$. As indicated in previous chapters, finding a policy which maximizes this *average value across states* is not necessarily the same as finding a policy which *dominates any other policy in every state*. But in most practical cases, $J(\\pi)$ is a very reasonable and interesting proxy for optimality.\n",
    "\n",
    "Recall that $V^\\pi(s) = Q^\\pi(s,\\pi(s))$. \n",
    "So, for a given function $Q$, instead of looking for $\\pi(s) \\in \\arg\\max_{a \\in A} \\left[Q(s,a)\\right], \\forall s\\in S$, we can redefine *greediness* and look for \n",
    "$$\\pi \\in \\arg\\max_{a \\in A} \\mathbb{E}_{s\\sim \\rho_0} [Q(s,\\pi(s))] = J_Q(\\pi).$$\n",
    "If $Q=Q^*$, then a maximizer of this quantity is a maximizer of $J(\\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5b609-b6ea-4295-b866-e9979d7b192f",
   "metadata": {},
   "source": [
    "Now if $\\pi$ is a parameterized function $\\pi_w$, then one can try to approximate $\\pi_n \\in \\mathcal{G}Q_n$ by taking gradient steps on $J_{Q_n}(\\pi_w)$. This is the key idea behind deterministic policy gradient algorithms.\n",
    "\n",
    "It relies on the deterministic policy gradient theorem, introduced by Silver at al (2014) in the **[Deterministic Policy Gradient Algorithms](https://proceedings.mlr.press/v32/silver14.html)** paper.\n",
    "\n",
    "Let us write $J(w) = J(\\pi_w)$ for a parametric policy $\\pi_w$.  \n",
    "Write also $\\rho^{\\pi_w}(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t p(S_t=s|\\rho_0,\\pi_w)$ for all $s \\in S$ the state occupancy measure of $\\pi_w$ given $\\rho_0$.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Deterministic policy gradient theorem**  \n",
    "Consider a deterministic policy $\\pi_w: S\\rightarrow A$ interacting with an MDP $(S, A, p, r)$ with a starting state distribution $\\rho_0$.  \n",
    "We will drop the $w$ subscripts wherever unambiguous, to improve readability.    \n",
    "If $p(s,a)$, $\\nabla_a p(s'|s,a)$, $r(s,a)$, $\\nabla_a r(s,a)$, $\\rho_0(s)$, $\\pi_w(s)$, and $\\nabla_w\\pi_w(s)$ all exist and are continuous in $(s,a,s')$, then \n",
    "$$\\nabla_w J(w) = \\mathbb{E}_{s\\sim \\rho^{\\pi}} \\left[ \\nabla_a Q^{\\pi}(s,a)|_{a=\\pi(s)} \\cdot \\nabla_w \\pi_w(s) \\right].$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b1ab5-9893-4756-925c-ac73866bb3cf",
   "metadata": {},
   "source": [
    "Note that $\\nabla_w \\pi_w(s)$ is a Jacobian matrix where each column is the derivative of an action variable with respect to the parameters $w$.\n",
    "\n",
    "Rewriting this theorem with partial derivatives, we have:\n",
    "$$\\nabla_w J(w) = \\mathbb{E}_{s\\sim \\rho^{\\pi}} \\left[ \\frac{\\partial Q^{\\pi}(s,a)}{\\partial a}(s,\\pi(s)) \\cdot \\frac{\\partial \\pi(s)}{\\partial w}(s) \\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38c973-9418-444b-85d6-bac91587ad41",
   "metadata": {},
   "source": [
    "This theorem looks like the chain rule applied to $J(\\pi_w)$ but it's actually a bit more than that.\n",
    "\n",
    "Let us write $J(w)=J(\\pi_w)$ again.\n",
    "\\begin{align*}\n",
    "J(w) &= \\mathbb{E}_{s\\sim \\rho_0} [V^{\\pi_w}(s)]\\\\\n",
    " &= \\mathbb{E}_{s\\sim \\rho_0} [Q^{\\pi_w}(s,\\pi_w(s))]\n",
    "\\end{align*}\n",
    "\n",
    "Let us take the gradient of this term with respect to $w$.\n",
    "$$\\nabla_w J(w) = \\mathbb{E}_{s\\sim \\rho_0} \\left[ \\frac{\\partial Q^{\\pi_w}(s,\\pi_w(s))}{\\partial w} \\right].$$\n",
    "\n",
    "If we had had a fixed $Q$ instead of $Q^{\\pi_w}$ in the expression above, we could have used the chain rule and we could have written:\n",
    "$$\\frac{\\partial Q(s,\\pi_w(s))}{\\partial w}  = \\frac{\\partial Q(s,a)}{\\partial a}(s,\\pi_w(s)) \\frac{\\partial \\pi_w(s)}{\\partial w}(s).$$\n",
    "\n",
    "But we don't have a fixed $Q$, and as soon as $w$ changes infinitesimally, $Q^{\\pi_w}$ changes too, so this chain rule is not so straightforward.\n",
    "\n",
    "The full proof of the deterministic policy gradient theorem is in the appendix of the aforementionned paper and we will not recall it here. In short, the derivation implies unfolding the sum over times steps of reward random variables in $Q^\\pi$, which leads to the introduction of $\\rho^\\pi$.  \n",
    "\n",
    "Instead, we will try to provide intuition as to why this gradient ascent direction makes sense in an AVI context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be1639c-f423-4a07-a09a-eaddd7ab26e8",
   "metadata": {},
   "source": [
    "## Connecting the DPG and AVI\n",
    "\n",
    "**The intuition.**  \n",
    "Suppose we have a current $Q_n$ in the AVI sequence, and we are searching for $\\pi_n$, with the intention to define $Q_{n+1}$. Then, for each visited state $s$, $\\pi_n(s)$ is a $Q_n$-greedy action $\\pi_n(s) \\in \\arg\\max_{a} Q_n(s,a)$. So, in each state, given a current $\\pi_w$, the policy parameters should move in the direction of $\\nabla_w Q_n(s,\\pi_w(s))$. Since this should be true in all states visited by $\\pi_w$, averaging these gradients according to $\\rho^{\\pi_w}$ makes sense.\n",
    "\n",
    "**Understanding the deterministic policy gradient theorem: where $\\infty$-horizon improvement and one-step greediness coincide.**  \n",
    "Let $\\pi_w$ be the current policy in the AVI process. Then if there is no approximation error, $Q_n = Q^{\\pi_w}$. We want to find a policy that dominates over $\\pi_w$. What the policy gradient theorem tells us is that the overall improvement step $\\nabla_w J(w)$ of $\\pi_w$ is found using gradients derived from the 1-step-lookahead value function $Q_n$ with respect to $\\pi_w$. In other words, in $w$ specifically, the deterministic policy gradient $\\nabla_w J(w)$ coincides with $\\mathbb{E}_{s\\sim \\rho^{\\pi_w}} [\\nabla_w Q_n(s,\\pi_w(s))]$. But this is only true in $w$, because of the tight coupling between $\\rho^{\\pi_w}$, $Q_n$, and $\\pi_w$. Once the gradient step is taken, both $\\rho^{\\pi_w}$ and $Q^{\\pi_w}$ change and need to be updated to perform future gradient steps: the gradient estimate can only use $Q_n$ because it was an estimate of $Q^{\\pi_w}$. \n",
    "\n",
    "**Nuts and bolts of DPG.**   \n",
    "Yet, provided we trust $Q_n$ to be close enough to $Q^{\\pi_w}$ and provided we can draw from a distribution close enough to $\\rho^{\\pi_w}$, this enables building a Monte Carlo estimator of $\\nabla_w J(w)$ by drawing samples according to $\\rho^{\\pi_w}$, summing the corresponding values of $Q_n(s,\\pi_w(s))$ and taking the gradient with respect to $w$.\n",
    "In practice, we often have a replay buffer which is representative not only of $\\rho^{\\pi_w}$, but rather of a mix of successive $\\rho^{\\pi_w}$. Nonetheless, this replay buffer will enable defining a loss function on the policy. Consequently, given a replay buffer distribution $\\rho_n$ at iteration $n$, we can redefine the AVI sequence as:\n",
    "\n",
    "$$L^\\pi_n(w) = \\mathbb{E}_{\\rho_n} \\left[ Q_n(s,\\pi_w(s)) \\right]$$\n",
    "$$w_n = w_{n-1} +\\alpha \\nabla_w L^\\pi_n(w_{n-1})$$\n",
    "$$\\pi_n = \\pi_{w_n}$$\n",
    "$$L^Q_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho_n}\\left[ \\left( Q(s,a;\\theta) - G^{\\pi_n}_1(s,a,Q_n) \\right)^2 \\right]$$\n",
    "$$\\theta_{n+1} \\in \\arg\\min_\\theta L^Q_n(\\theta)$$\n",
    "$$Q_{n+1} = Q_{\\theta_n}$$\n",
    "\n",
    "Recall that the loss $L^Q_n$ on the Q-function's parameters is actually an off-policy loss, requiring only that $\\rho_n$ covers states and actions which are likely to be encountered by $\\pi_n$.\n",
    "\n",
    "In turn, this provides us with a direct way to implement a general **[Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971)** or DDPG (Lillicrap et al., 2016) algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69660d3d-c9cf-4ac5-a32e-1447356f7dca",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "\n",
    "Recall that (as in the previous chapter), a single gradient step makes for a poor function approximator. To compensate for this, we previously introduced two mechanisms: either take several gradient steps with respect to a given *target* network (we did it for $Q$ but the same idea applies to $\\pi$), or implement a moving average. DDPG implements the latter. Define $\\theta'$ and $w'$ the target networks' parameters, then $G^{\\pi_n}_1(s,a,Q_n)$ in the Q-function loss becomes $G^{\\pi_{w'}}_1(s,a,Q_{\\theta'})$. \n",
    "\n",
    "So, after each drawn mini-batch:\n",
    "$$w \\leftarrow w + \\alpha_w \\nabla_w L^\\pi_n(w),$$\n",
    "$$\\theta \\leftarrow \\theta - \\alpha_\\theta \\nabla_\\theta L^Q_n(\\theta).$$\n",
    "And the target networks are updated according to:\n",
    "$$w' \\leftarrow \\tau w + (1-\\tau) w',$$\n",
    "$$\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\theta'.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8beed9-7b0a-49a5-8033-e1992722ccaf",
   "metadata": {},
   "source": [
    "In order to use libraries like `pytorch` which perform stochastic gradient descent (and not ascent), we redefine the policy update loss as $-Q(s,\\pi(s))$.\n",
    "\n",
    "Finally, we need to define the behavior policy. The policy update requires states to be drawn according to $\\rho^\\pi$, but the Q-function update requires to test all actions in the encountered states. Since we work with continuous actions, we turn to an exploration policy consisting of adding noise to the deterministic policy's action. Historically, this noise was a time-correlated Ornstein-Uhlenbeck noise but this later appeared to be unnecessary in practice and a simple normal distribution suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ab0dd-bb8e-4fc5-ae80-e1c6703d3fd1",
   "metadata": {},
   "source": [
    "Overall, the pseudo-code of DDPG is:\n",
    "\n",
    "```\n",
    "Initialize theta=theta' and w=w' \n",
    "Initialize replay buffer RB\n",
    "s = env.init()\n",
    "loop:\n",
    "   Pick a = pi_w(s) + noise\n",
    "   s',r = env.step(a)\n",
    "   RB.append(s,a,r,s')\n",
    "   minibatch = RB.sample()\n",
    "   actor_loss(w) = MSE(Q(s,pi(s,w),theta)\n",
    "   actor_loss.gradient_ascent_step()\n",
    "   target_value(s,a) = r + gamma Q(s',pi(s',w'),theta')\n",
    "   critic_loss(theta) = MSE(Q(s,a,theta), target_value(s,a))\n",
    "   critic_loss.gradient_descent_step()\n",
    "   s=s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d3a91-216a-4f70-bf1f-a64987161d19",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Declare a class for a Q-function neural network. Use two hidden layers with 256 neurons each.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b635b7-11e6-4310-aec7-e8ff8f0364b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Warning, this class only works for vector-shaped inputs (for images, it requires adjustments).\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796b13d-45a3-473f-9857-71f1a4565819",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Declare a class for a policy neural network. Use two hidden layers with 256 neurons each.  \n",
    "\\[Optional\\] Include an option for inputing upper and lower bounds to enable action spaces not centered on zero, and store and `action_scale` and an `action_bias` parameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7712de-b4b8-47cf-907b-f2622ced47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Warning, this class only works for vector-shaped inputs (for images, it requires adjustments).\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, action_dim)\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110cd7f-bdcc-450a-944e-a64714b2ae42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write a class that implements the DDPG pseudo-code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf331fa-328b-4e2d-856f-17b26eca6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from solutions.replay_buffer2 import ReplayBuffer\n",
    "from tqdm import trange\n",
    "\n",
    "class ddpg_agent:\n",
    "    def __init__(self, config, value_network, policy_network):\n",
    "        # networks\n",
    "        device = \"cuda\" if next(value_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(value_network.parameters()).dtype\n",
    "        self.Qfunction = value_network\n",
    "        self.Q_target = deepcopy(self.Qfunction).to(device)\n",
    "        self.pi = policy_network\n",
    "        self.pi_target = deepcopy(self.pi).to(device)\n",
    "        # parameters\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size, device)\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.Q_optimizer = torch.optim.Adam(list(self.Qfunction.parameters()), lr=lr)\n",
    "        self.pi_optimizer = torch.optim.Adam(list(self.pi.parameters()), lr=lr)\n",
    "        self.tau = config['tau'] if 'tau' in config.keys() else 0.005\n",
    "        self.exploration_noise = config['exploration_noise'] if 'exploration_noise' in config.keys() else 0.005\n",
    "        self.delay_learning = config['delay_learning'] if 'delay_learning' in config.keys() else 1e4\n",
    "        self.tqdm_disable = config['tqdm_disable'] if 'tqdm_disable' in config.keys() else True\n",
    "        self.disable_episode_report = config['disable_episode_report'] if 'disable_episode_report' in config.keys() else True\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello world\")\n",
    "    def train(self, env, max_steps):\n",
    "        x,_ = env.reset()\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        episode_return = []\n",
    "\n",
    "        for time_step in trange(int(max_steps), disable=self.tqdm_disable):\n",
    "            # step (policy + noise), add to rb\n",
    "            if time_step > self.delay_learning:\n",
    "                with torch.no_grad():\n",
    "                    a = self.pi(torch.tensor(x,dtype=self.scalar_dtype))\n",
    "                    a += torch.normal(0, self.pi.action_scale * self.exploration_noise)\n",
    "                    a = a.cpu().numpy().clip(env.action_space.low, env.action_space.high)\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "            y, r, done, trunc, _ = env.step(a)\n",
    "            self.memory.append(x,a,r,y,done)\n",
    "            episode_cum_reward += r\n",
    "            \n",
    "            # gradient step\n",
    "            if time_step > self.delay_learning:\n",
    "                X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "                ## Qfunction update\n",
    "                with torch.no_grad():\n",
    "                    next_actions = self.pi_target(Y)\n",
    "                    QYA = self.Q_target(Y, next_actions)\n",
    "                    #target = torch.addcmul(R, 1-D, QY, value=self.gamma)\n",
    "                    target = R + self.gamma * (1-D) * QYA.view(-1)\n",
    "                QXA = self.Qfunction(X, A).view(-1)\n",
    "                Qloss = F.mse_loss(QXA,target)\n",
    "                self.Q_optimizer.zero_grad()\n",
    "                Qloss.backward()\n",
    "                self.Q_optimizer.step()\n",
    "                ## policy update\n",
    "                pi_loss = -self.Qfunction(X, self.pi(X)).mean()\n",
    "                self.pi_optimizer.zero_grad()\n",
    "                pi_loss.backward()\n",
    "                self.pi_optimizer.step()\n",
    "                \n",
    "                # target networks update\n",
    "                for param, target_param in zip(self.pi.parameters(), self.pi_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                for param, target_param in zip(self.Qfunction.parameters(), self.Q_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                \n",
    "            # if done, print episode info\n",
    "            if done or trunc:\n",
    "                x, _ = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                if not self.disable_episode_report:\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", buffer size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", episode return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          sep='')\n",
    "                episode += 1\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                x=y\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e23a2-3283-44a4-b6e3-393e503efb08",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Test your code on Gymnasium's [MuJoCo inverted pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) or [bipedal walker](https://gymnasium.farama.org/environments/box2d/bipedal_walker/) environments.  \n",
    "Caveat: training might be long!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c306372-2fe6-42a9-83ec-a9b2f72f515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "#env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'buffer_size': 1e6,\n",
    "          'learning_rate': 3e-4,\n",
    "          'batch_size': 256,\n",
    "          'tau': 0.005,\n",
    "          'delay_learning': 1e4,\n",
    "          'exploration_noise': .1,\n",
    "          'tqdm_disable': False\n",
    "         }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Qfunction = QNetwork(env).to(device)\n",
    "policy = policyNetwork(env).to(device)\n",
    "\n",
    "agent = ddpg_agent(config, Qfunction, policy)\n",
    "episode_returns = agent.train(env, 2e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dfa5a4-1f5d-48e1-829b-cf0d4f8c5442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(episode_returns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4a735-5887-4558-80f9-13dd2cac56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "#test_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array_list\")\n",
    "test_env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array_list\")\n",
    "scalar_dtype = next(policy.parameters()).dtype\n",
    "s,_ = test_env.reset()\n",
    "with torch.no_grad():\n",
    "    for t in range(1000):\n",
    "        a = policy(torch.tensor(s,dtype=scalar_dtype)).numpy()\n",
    "        s2,r,d,trunc,_ = test_env.step(a)\n",
    "        s = s2\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "save_video(test_env.render(), \"videos\", fps=test_env.metadata[\"render_fps\"], name_prefix=\"ddpg_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322d7b5-d8a8-48cd-ab04-9a4edf6b475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/ddpg_policy-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63902de1-41e9-4455-a366-900b483f04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "#test_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "test_env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
    "scalar_dtype = next(policy.parameters()).dtype\n",
    "returns = []\n",
    "for _ in trange(20):\n",
    "    cumulated_reward = 0\n",
    "    s,_ = test_env.reset()\n",
    "    with torch.no_grad():\n",
    "        for t in range(1000):\n",
    "            a = policy(torch.tensor(s,dtype=scalar_dtype)).numpy()\n",
    "            s2,r,d,trunc,_ = test_env.step(a)\n",
    "            cumulated_reward += r\n",
    "            s = s2\n",
    "            if d:\n",
    "                break\n",
    "    returns.append(cumulated_reward)\n",
    "\n",
    "print(np.mean(returns))\n",
    "print(np.std(returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a171c-4a5b-4a96-8f89-5fcd9bc3b42d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Modify the `Qnetwork` and `policyNetwork` classes above to take images as inputs and run DDPG on Gymnasium's [car racing environment](https://gymnasium.farama.org/environments/box2d/car_racing/).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888b275-e67a-453f-9cd8-62ba20688d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/no_solution_yet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716895b6-c7b8-49cf-9831-122ec87ecff3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (discussion):**  \n",
    "Is DDPG off-policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71809c80-28fb-410d-bc33-7a3b8e3bae8a",
   "metadata": {},
   "source": [
    "# TD3: improving value function approximation in DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e218752-d5a7-41c0-b6fc-6d06af92924b",
   "metadata": {},
   "source": [
    "Just like DQN and other VI-based algorihtms, DDPG is prone to overestimation of the Q-function. This was improved in the discrete actions domain by [Double Q-learning](https://arxiv.org/abs/1509.06461) and its extension [Double DQN](https://ojs.aaai.org/index.php/AAAI/article/view/10295), taking the greedy action with respect to the current value network, while its value is taken using the target network.\n",
    "\n",
    "Building on this idea, Fujimoto et al. introduced [Twin Delayed DDPG](https://arxiv.org/pdf/1802.09477.pdf) also known as TD3, as an improvement on DDPG which:\n",
    "- defines double critics (as in DDQN),\n",
    "- defines a new update target called *clipped Q-learning*,\n",
    "- enables better fitting of $Q^\\pi$ by $Q_n$, by performing two gradient steps on $\\theta$ before each gradient step on $w$ and each target network update.\n",
    "These three items motivate the name of the algorithm.\n",
    "\n",
    "The clipped Q-learning target is:\n",
    "$$y = r + \\gamma \\min_{i\\in\\{1,2\\}} Q(s', \\pi_w(s'); \\theta'_i)$$\n",
    "\n",
    "Additionally, TD3 improves generalization across actions by adding a clipped noise term to the actions in the target. This can be seen as a way to promote value function smoothness with respect to continuous actions (it is a very naive form of data augmentation). Hence, the target value above is changed to:\n",
    "$$y = r + \\gamma \\min_{i\\in\\{1,2\\}} Q(s', \\pi_w(s') + \\epsilon; \\theta'_i)$$\n",
    "where $\\epsilon$ is drawn according to a clipped, centered Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdf8c4-6146-4707-93e2-8f865678f063",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (discussion):**  \n",
    "Does this make TD3 close to SARSA?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453e49a-28a8-4f00-8dd2-1d9ad17c1b46",
   "metadata": {},
   "source": [
    "Overall, the pseudo-code of TD3 is:\n",
    "\n",
    "```\n",
    "Initialize theta1=theta1', theta2=theta2' and w=w' \n",
    "Initialize replay buffer RB\n",
    "s = env.init()\n",
    "loop:\n",
    "   Pick a = pi_w(s) + noise\n",
    "   s',r = env.step(a)\n",
    "   RB.append(s,a,r,s')\n",
    "   minibatch = RB.sample()\n",
    "   next_action = pi(s',w') + clipped_noise\n",
    "   target_value(s,a) = r + gamma min [ Q(s',next_action,theta1'), Q(s',next_action,theta2') ]\n",
    "   critic1_loss(theta) = MSE(Q(s,a,theta1), target_value(s,a))\n",
    "   critic2_loss(theta) = MSE(Q(s,a,theta2), target_value(s,a))\n",
    "   critic_loss = critic1_loss + critic2_loss\n",
    "   critic_loss.gradient_descent_step()\n",
    "   only one step out of two:\n",
    "      actor_loss(w) = MSE(Q1(s,pi(s,w),theta)\n",
    "      actor_loss.gradient_ascent_step()\n",
    "      target networks update\n",
    "   s=s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03241e8-d021-40f6-919b-5dd8f82134b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (discussion):**  \n",
    "Implement the pseudo-code above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983a562-344c-434d-a5f7-06880b9676c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from solutions.replay_buffer2 import ReplayBuffer\n",
    "from tqdm import trange\n",
    "\n",
    "class td3_agent:\n",
    "    def __init__(self, config, value_network1, value_network2, policy_network):\n",
    "        # networks\n",
    "        self.device = \"cuda\" if next(value_network1.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(value_network1.parameters()).dtype\n",
    "        self.Qfunction1 = value_network1\n",
    "        self.Qfunction2 = value_network2\n",
    "        self.Q1_target = deepcopy(self.Qfunction1).to(device)\n",
    "        self.Q2_target = deepcopy(self.Qfunction2).to(device)\n",
    "        self.pi = policy_network\n",
    "        self.pi_target = deepcopy(self.pi).to(device)\n",
    "        # parameters\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size, device)\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.Q_optimizer = torch.optim.Adam(list(Qfunction1.parameters()) + list(Qfunction2.parameters()), lr=lr)\n",
    "        self.pi_optimizer = torch.optim.Adam(list(self.pi.parameters()), lr=lr)\n",
    "        self.tau = config['tau'] if 'tau' in config.keys() else 0.005\n",
    "        self.exploration_noise = config['exploration_noise'] if 'exploration_noise' in config.keys() else 0.005\n",
    "        self.delay_learning = config['delay_learning'] if 'delay_learning' in config.keys() else 1e4\n",
    "        self.action_noise_scale = config['action_noise_scale'] if 'action_noise_scale' in config.keys() else 0.2\n",
    "        self.action_noise_clip = config['action_noise_clip'] if 'action_noise_clip' in config.keys() else 0.5\n",
    "        self.policy_update_freq = config['policy_update_freq'] if 'policy_update_freq' in config.keys() else 2\n",
    "        self.tqdm_disable = config['tqdm_disable'] if 'tqdm_disable' in config.keys() else True\n",
    "        self.disable_episode_report = config['disable_episode_report'] if 'disable_episode_report' in config.keys() else True\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello world\")\n",
    "    def train(self, env, max_steps):\n",
    "        x,_ = env.reset()\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        episode_return = []\n",
    "\n",
    "        for time_step in trange(int(max_steps), disable=self.tqdm_disable):\n",
    "            # step (policy + noise), add to rb\n",
    "            if time_step > self.delay_learning:\n",
    "                with torch.no_grad():\n",
    "                    a = self.pi(torch.tensor(x,dtype=self.scalar_dtype))\n",
    "                    a += torch.normal(0, self.pi.action_scale * self.exploration_noise)\n",
    "                    a = a.cpu().numpy().clip(env.action_space.low, env.action_space.high)\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "            y, r, done, trunc, _ = env.step(a)\n",
    "            self.memory.append(x,a,r,y,done)\n",
    "            episode_cum_reward += r\n",
    "            \n",
    "            # gradient step\n",
    "            if time_step > self.delay_learning:\n",
    "                X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "                ## Qfunction update\n",
    "                with torch.no_grad():\n",
    "                    # next action with noise\n",
    "                    noise = torch.randn_like(A, device=self.device) * self.action_noise_scale\n",
    "                    clipped_noise = noise.clamp(-self.action_noise_clip, self.action_noise_clip) * self.pi_target.action_scale\n",
    "                    next_actions = self.pi_target(Y) + clipped_noise\n",
    "                    next_actions=next_actions.clamp(env.action_space.low[0], env.action_space.high[0])\n",
    "                    # clipped q-learning target\n",
    "                    Q1YA = self.Q1_target(Y, next_actions)\n",
    "                    Q2YA = self.Q2_target(Y, next_actions)\n",
    "                    min_QYA = torch.min(Q1YA, Q2YA)\n",
    "                    target = R + self.gamma * (1-D) * min_QYA.view(-1)\n",
    "                # double q-network update\n",
    "                Q1XA = self.Qfunction1(X, A).view(-1)\n",
    "                Q2XA = self.Qfunction2(X, A).view(-1)\n",
    "                Q1loss = F.mse_loss(Q1XA,target)\n",
    "                Q2loss = F.mse_loss(Q2XA,target)\n",
    "                Qloss = Q1loss + Q2loss\n",
    "                self.Q_optimizer.zero_grad()\n",
    "                Qloss.backward()\n",
    "                self.Q_optimizer.step()\n",
    "                ## policy update\n",
    "                if time_step % self.policy_update_freq ==0:\n",
    "                    pi_loss = -self.Qfunction1(X, self.pi(X)).mean()\n",
    "                    self.pi_optimizer.zero_grad()\n",
    "                    pi_loss.backward()\n",
    "                    self.pi_optimizer.step()\n",
    "                    # target networks update\n",
    "                    for param, target_param in zip(self.pi.parameters(), self.pi_target.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    for param, target_param in zip(self.Qfunction1.parameters(), self.Q1_target.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    for param, target_param in zip(self.Qfunction2.parameters(), self.Q2_target.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            \n",
    "            # if done, print episode info\n",
    "            if done or trunc:\n",
    "                x, _ = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                if not self.disable_episode_report:\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", buffer size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", episode return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          sep='')\n",
    "                episode += 1\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                x=y\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880228d4-40ab-4434-bd9b-6d201649de15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Test your code on Gymnasium's [MuJoCo inverted pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) or [bipedal walker](https://gymnasium.farama.org/environments/box2d/bipedal_walker/) environments.  \n",
    "Caveat: training might be long!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75baa677-5de7-4197-820a-594ac311db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'buffer_size': 1e6,\n",
    "          'learning_rate': 3e-4,\n",
    "          'batch_size': 256,\n",
    "          'tau': 0.005,\n",
    "          'delay_learning': 1e4,\n",
    "          'exploration_noise': .1,\n",
    "          'action_noise_scale': 0.2,\n",
    "          'action_noise_clip\"': 0.5,\n",
    "          'policy_update_freq': 2,\n",
    "          'tqdm_disable': False\n",
    "         }\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Qfunction1 = QNetwork(env).to(device)\n",
    "Qfunction2 = QNetwork(env).to(device)\n",
    "policy = policyNetwork(env).to(device)\n",
    "\n",
    "agent = td3_agent(config, Qfunction1, Qfunction2, policy)\n",
    "episode_returns = agent.train(env, 2e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20761b43-850f-47a4-a0d9-91562c0f06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(episode_returns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ce9f9-1f66-41cc-b919-bc84fdce6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "#test_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array_list\")\n",
    "test_env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array_list\")\n",
    "scalar_dtype = next(policy.parameters()).dtype\n",
    "s,_ = test_env.reset()\n",
    "with torch.no_grad():\n",
    "    for t in range(1000):\n",
    "        a = policy(torch.tensor(s,dtype=scalar_dtype)).numpy()\n",
    "        s2,r,d,trunc,_ = test_env.step(a)\n",
    "        s = s2\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "save_video(test_env.render(), \"videos\", fps=test_env.metadata[\"render_fps\"], name_prefix=\"ddpg_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbd3ad-eb4f-4a72-a7e9-ea45518a3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/ddpg_policy-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821ecee-d661-436f-95c8-5c4dfbc20024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "#test_env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "test_env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
    "scalar_dtype = next(policy.parameters()).dtype\n",
    "returns = []\n",
    "for _ in trange(50):\n",
    "    cumulated_reward = 0\n",
    "    s,_ = test_env.reset()\n",
    "    with torch.no_grad():\n",
    "        for t in range(1000):\n",
    "            a = policy(torch.tensor(s,dtype=scalar_dtype)).numpy()\n",
    "            s2,r,d,trunc,_ = test_env.step(a)\n",
    "            cumulated_reward += r\n",
    "            s = s2\n",
    "            if d:\n",
    "                break\n",
    "    returns.append(cumulated_reward)\n",
    "\n",
    "print(np.mean(returns))\n",
    "print(np.std(returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836a7bd-a664-424d-aa02-b13172814131",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC): the maximum entropy principle within AVI\n",
    "\n",
    "We now depart from the deterministic policies we have used so far. We still consider AVI sequences, but now with stochastic policies for continuous actions.\n",
    "\n",
    "Soft Actor-Critic algorithms correspond to a series of papers:  \n",
    "[Reinforcement Learning with Deep Energy-Based Policies](https://arxiv.org/abs/1702.08165) (ICML 2017)  \n",
    "[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290) (ICML 2018)  \n",
    "[Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905) (arXiv only, 2019)  \n",
    "One should also mention the complementary paper:  \n",
    "[Soft Actor-Critic for Discrete Action Settings](https://arxiv.org/abs/1910.07207) (arXiv only, 2019)  \n",
    "\n",
    "The most comprehensive presentation is given in [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905), which forms the basis for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b5ea0-45d7-4e9d-8b2b-7f1621bc4892",
   "metadata": {},
   "source": [
    "## Why maximize the policy's entropy?\n",
    "\n",
    "As we saw, DDPG (and TD3) is a straightforward way to extend DQN to continuous actions. In a nutshell, it uses an off-policy critic to learn $Q^\\pi$ and takes gradient steps in the direction of the deterministic policy gradient. \n",
    "\n",
    "In large state and action spaces, DDPG-like algorithms are somehow inefficient as they rely on this fixed exploration policy, and a delicate interplay between the actor and the critic(s). Optimizing a deterministic policy in an actor-critic architecture with continuous actions is prone to instabilities and errors. For example, suppose that in a given state, two actions are deemed equally good at a certain stage of the AVI process. DDPG's actor will \"fall\" towards one or the other, mostly depending on noise in the minibatches' distribution. If we are unlucky and it turns out the optimal action actually was the other one, it might be difficult to escape the local minimum originally found and converge to the true optimal policy.\n",
    "\n",
    "Stochastic policies offer an alternative, by dispatching probability mass across the different actions in each state. Intuitively, instead of encoding the knowledge of an $\\arg\\max$ in a given state, they can encode the knowledge of a ranking among actions. But for this, we need to impose that they retain a certain level of entropy so that all probability mass does not end up concentrated on a single state. \n",
    "Transferring probability mass between actions is not as abrupt as jumping from one deterministic value to the other, so we expect the optimization process on such policies to be smoother. Hence we are interested in stochastic policies that maintain a certain level of action distribution entropy in each state.\n",
    "\n",
    "Such policies have been shown to be beneficial to robustness to model mispecifications, to improve gradient descent convergence by smoothing out the loss landscape, and to promote exploration.\n",
    "\n",
    "The maximum entropy objective for policies is:\n",
    "$$\\pi^* = \\arg\\max_\\pi \\sum_t \\gamma^t \\mathbb{E}_{s_t,a_t} \\left[r \\left(s_t,a_t\\right) + \\alpha \\mathcal{H}\\left(\\pi\\left(s_t\\right)\\right) \\right]$$\n",
    "where $\\alpha$ is a temperature parameter, trading-off rewards for action distribution entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974055fe-a7e0-41b7-8806-260023e67127",
   "metadata": {},
   "source": [
    "## Soft Policy Iteration\n",
    "\n",
    "From the maximum entropy objective function, one can define the soft value function:\n",
    "$$V^\\pi(s) = \\sum_t \\gamma^t \\mathbb{E}_{s_t,a_t} \\left[r \\left(s_t,a_t\\right) + \\alpha \\mathcal{H}\\left(\\pi\\left(s_t\\right)\\right) | s_0=s, \\pi \\right]$$\n",
    "\n",
    "One can derive the relationship between $V$ and $Q$ functions and, in turn, the policy evaluation operator:\n",
    "$$T^\\pi Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(\\cdot|s,a)} \\left[ V(s') \\right],$$\n",
    "$$V(s) = \\mathbb{E}_{a\\sim \\pi(s)} \\left[ Q(s,a) - \\alpha \\log \\pi(a|s) \\right].$$\n",
    "\n",
    "So $$T^\\pi Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(\\cdot|s,a)} \\mathbb{E}_{a'\\sim \\pi(s')} \\left[ Q(s',a') - \\alpha \\log \\pi(a'|s') \\right].$$\n",
    "\n",
    "This operator, like the classical one, is a contraction mapping. So repeatedly applying it defines a sequence $Q_{k+1} = T^\\pi Q_k$ which converges to the **soft Q-function** of $\\pi$.\n",
    "\n",
    "Given the soft Q-function $Q^{\\pi_n}$ of policy $\\pi_n$, one can consider the stochastic policy $\\pi_{n+1}(s)$ that puts probability mass on each action in proportion to $\\exp(\\frac{1}{\\alpha}Q^{\\pi_n}(s,a))$. Then, given a family $\\Pi$ of stochastic policies (including $\\pi_n$) one can define:\n",
    "$$\\pi_{n+1}(s) = \\arg\\min_{\\pi \\in \\Pi} D_{KL} \\left( \\pi(s) \\Bigg|\\Bigg| \\frac{\\exp(\\frac{1}{\\alpha}Q^{\\pi_n}(s,a))}{Z^{\\pi_n}(s)} \\right),$$\n",
    "where $Z^{\\pi_n}(s)$ is a normalizing term so that the right-hand side of the divergence is indeed a probability density function. Then one can prove that $Q^{\\pi_{n+1}} \\geq Q^{\\pi_n}$. The proof is very similar to that of the policy improvement phase in policy iteration. \n",
    "\n",
    "One could wonder why it is necessary to take a temperature equal to the entropy trade-off coefficient $\\alpha$ when defining $\\pi_{n+1}$ (after all, $\\pi_{n+1}$ is \"just\" the projection on $\\Pi$ of $Q^{\\pi_n}$'s softmax, with a temperature). Interestingly, the proof that $Q^{\\pi_{n+1}} \\geq Q^{\\pi_n}$ builds on this assumption.\n",
    "\n",
    "In turn, this enables defining a soft policy iteration algorithm which converges to a policy $\\pi^*$ whose soft Q-function $Q^{\\pi^*}$ dominates over any other policy in $\\Pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff43ec6-b45b-4006-8309-5aa1423ff7f2",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic\n",
    "\n",
    "As for previous algorithms, we wish to introduce function approximators for the value function and the policy. We will write $Q(s,a;\\theta)$ for Q-function approximators, and $\\pi_w(s)$ for parameterized policies. We will also define target network parameters $\\theta'$.\n",
    "\n",
    "We can write an off-policy loss on the value function's parameters:\n",
    "$$L^Q(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a)\\sim \\rho} \\left[ \\left( r(s,a) + \\gamma \\mathbb{E}_{\\substack{s'\\sim p(\\cdot|s,a)\\\\a'\\sim\\pi_{w}(s')}}\\left[ Q(s',a';\\theta') - \\alpha \\log \\pi_{w}(a'|s') \\right] - Q(s,a;\\theta) \\right)^2 \\right].$$\n",
    "\n",
    "And the loss on the policy is the KL divergence defined earlier, averaged across states in the replay buffer:\n",
    "$$L^\\pi(w) = \\mathbb{E}_{\\substack{s\\sim \\rho\\\\a\\sim\\pi_w(s)}} \\left[ \\alpha \\log \\pi_w(a|s) - Q(s,a;\\theta) \\right]. $$\n",
    "\n",
    "Taking the gradient $\\nabla_w L^\\pi(w)$ is feasible thanks to the reparametrization trick. Let us take a family of distributions $\\Pi$ such that drawing $a$ from $\\pi_w(s)$ is equivalent to drawing $\\epsilon$ from a fixed distribution (eg. Gaussian) and running $\\epsilon$ through function $f_w(\\epsilon,s)$ to obtain $a$. Then one has:\n",
    "$$L^\\pi(w) = \\mathbb{E}_{\\substack{s\\sim \\rho\\\\ \\epsilon\\sim\\mathcal{N}}} \\left[ \\alpha \\log \\pi_w(f_w(\\epsilon,s)|s) - Q(s,f_w(\\epsilon,s);\\theta) \\right], $$\n",
    "where each $\\log \\pi_w(f_w(\\epsilon,s)|s)$ is really the log-probability of each value of $\\epsilon$.\n",
    "\n",
    "When one looks closely at these loss functions, they directly extend DDPG's losses to the case of stochastic policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccc236-cbe1-4588-9368-4f65712ca870",
   "metadata": {},
   "source": [
    "This provides a direct algorithm, which can be implemented using double critics as:\n",
    "\n",
    "```\n",
    "Initialize theta1=theta1', theta2=theta2' and w\n",
    "Initialize replay buffer RB\n",
    "s = env.init()\n",
    "loop:\n",
    "   Sample a from pi(s,w) using the reparametrization trick\n",
    "   s',r = env.step(a)\n",
    "   RB.append(s,a,r,s')\n",
    "   minibatch = RB.sample()\n",
    "   next_action, next_log_prob = sample from pi(s',w)\n",
    "   next_value(s,a) = min [ Q(s',next_action,theta1'), Q(s',next_action,theta2') ] - alpha * next_log_prob\n",
    "   target_value(s,a) = r + gamma * next_value(s,a)\n",
    "   critic1_loss(theta) = MSE(Q(s,a,theta1), target_value(s,a))\n",
    "   critic2_loss(theta) = MSE(Q(s,a,theta2), target_value(s,a))\n",
    "   critic_loss = critic1_loss + critic2_loss\n",
    "   critic_loss.gradient_descent_step()\n",
    "   action, log_prob = sample from pi(s,w)\n",
    "   actor_loss(w) = alpha * log_prob - min [ Q(s,action,theta1'), Q(s,action,theta2') ] \n",
    "   actor_loss.gradient_descent_step()\n",
    "   target networks update\n",
    "   s=s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca3d54-63d2-48a5-af25-1341908f6861",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Adjust the previous `policyNetwork` class to encode a policy that relies on an internal Gaussian distribution. It predicts a mean and a log standard deviation. To draw actions, it uses the reparametrization trick and passes its samples through a tanh function to ensure the actions remain bounded.  \n",
    "Use the following tricks to ease your implementation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa5176-73cc-42d5-8ac6-ebf1cc16a706",
   "metadata": {},
   "source": [
    "**Trick 1**\n",
    "\n",
    "Consider a random variable $u$, of probability density function $\\mu(u|s)$.  \n",
    "Let $a = \\tanh(u)$.  \n",
    "Then $$\\log\\pi(a|s) = \\log\\mu(u|s) - \\sum_{i=1}^D \\log(1-\\tanh^2(u_i)),$$\n",
    "where $u_i$ is the $i$th element of $u$.\n",
    "\n",
    "**Trick 2**\n",
    "\n",
    "Standard deviations shouldn't grow or shrink unbounded. To keep them in check, we will assume no standard deviation is smaller than $10^-5$ and larger than $10^2$. To make sure the predicted $\\log\\sigma$ is within this interval, we run it through a $\\tanh$ function which we scale between $-5$ and $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e4c29-62d6-4b88-897c-68a3682aea59",
   "metadata": {},
   "source": [
    "The implementation below is freely adapted from the [CleanRL](https://docs.cleanrl.dev/rl-algorithms/sac/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d7d17-ab8f-4f18-8f4c-c341da533a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "class policy_network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.fc1 = nn.Linear(np.array(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, action_dim)\n",
    "        self.fc_logstd = nn.Linear(256, action_dim)\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed1f29-f999-4cd9-ac3b-419000330af1",
   "metadata": {},
   "source": [
    "Unfinished, please check [CleanRL](https://docs.cleanrl.dev/rl-algorithms/sac/) for a full implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003aab14-cb01-4088-b694-060385762ceb",
   "metadata": {},
   "source": [
    "## Going further: AVI under a minimal entropy constraint\n",
    "\n",
    "One can derive a version of SAC with adjustable temperature by casting the optimization problem as a contrained optimization one, instead of a regularized one.\n",
    "\n",
    "More on this later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
